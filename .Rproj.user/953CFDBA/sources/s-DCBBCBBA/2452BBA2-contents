---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

```{r}
#install.packages("mlbench")
#install.packages("lmtest")
library(readxl)
library(mlbench)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lmtest)
#install.packages("sandwich")
library(sandwich)
#install.packages("vegan")
library(vegan)

 cer <- read_excel("Book4.xlsx", skip = 1)
head(cer)
cer2<- cer
cer2$mfr<- as.factor(cer2$mfr)
head(cer)

cer$mfr<- ifelse(cer$mfr== "K", 1,0)
cer<- cer%>%select(-potass, -carbo, -sodium)

cer$shelf <- as.factor(cer$shelf) ; cer$name<- as.factor(cer$name)
head(cer)

```

```{r}
#Question 0.

#My datset is a collection of cereals with their name, whether or not they were manufactured by Kellog's or General Mills, calories per serving, sugars, what shelf they are placed on, and overall rating. I got this dataset from Kaggle. In total, there are 43 observations. 
```


```{r}
#Question 1.

#MANOVA
man1<-manova(cbind(rating,sugars, calories )~shelf, data=cer)

summary(man1)
#we see that the pvalue is less than .05 and so we reject the null hypothesis and continue on to univariate ANOVA

#univariate ANOVA
summary.aov(man1)
#we see that for rating and sugars the shelf position differs whereas the calories do not

diffmeans<- cer%>%group_by(shelf)%>%summarize(mean(rating),mean(sugars))
diffmeans
#we see the difference in means in both groups

pairwise.t.test(cer$sugars,cer$shelf,
                p.adj="none")
#we see that the middle and bottom and middle and top are significant 
pairwise.t.test(cer$rating,cer$shelf,
                p.adj="none")
#we see that the top and middle are significant

#number of tests: one manova, 3 anova and 6 t tests
.05/10
#bonferonni is .005
#after bonferonni the bottom and middle and top and middle are significant for sugars but rating is no longer significant

#type one error
1-(.95^10)
#type 1 error is .4012631



```
I think that this data has passed assumptions for random samples and independent observations. The data does not pass for multivariate normality of DV's since each group does not have greater than 25 counts. Based on this, it would be difficult for the data to pass other assumptions because the dataset it very small.
```{r}
#Question 2.


dists<-cer%>%select(sugars, calories)%>%dist()
adonis(dists~shelf,data=cer)

SST<- sum(dists^2)/43
SSW<-cer%>%group_by(shelf)%>%select(sugars,calories)%>%
do(d=dist(.[1:2],"euclidean"))%>%ungroup()%>%
summarize(sum(d[[1]]^2)/8 + sum(d[[2]]^2)/14+ sum(d[[3]]^2)/21)%>%pull

F_obs<-((SST-SSW)/2)/(SSW/40) 

Fs<-replicate(1000,{
new<-cer%>%mutate(shelf=sample(shelf)) #permute the species vector
SSW<-new%>%group_by(shelf)%>%select(sugars,calories)%>%
do(d=dist(.[1:2],"euclidean"))%>%ungroup()%>%
summarize(sum(d[[1]]^2)/8 + sum(d[[2]]^2)/14+ sum(d[[3]]^2)/21)%>%pull
((SST-SSW)/2)/(SSW/40) #calculate new F on randomized data
})
{hist(Fs,prob = T); abline(v=F_obs, col="red", add=T)}
mean(Fs>F_obs)

```
Null hypothesis is there is no difference in the mean distance or spread between groups. The alternative hypothesis is that there is a difference in mean distance or spread between groups.The pvalue is very small and so we would reject the null hypothesis. 

```{r}
#Question 3.

rating_c <- cer$rating - mean(cer$rating)
sugars_c <- cer$sugars - mean(cer$sugars)
calories_c<- cer$calories - mean(cer$calories)

#linear regresssion
fit<- lm(rating_c~calories_c*sugars_c, data = cer)
summary(fit)

#linear regression plot
cer%>%ggplot(aes(rating_c,sugars_c))+geom_point()+geom_smooth(method = 'lm',se=F)

#checking assumptions
resids<-fit$residuals; fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, col="red") #checks for linearity and homoskedacity


ggplot()+geom_histogram(aes(resids),bins=20) #normality
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line(aes(sample=resids), color='red') #normality

#regression results
coeftest(fit, vcov. = vcovHC(fit))
```
> - Intercept: Predicted rating for a cereal with sugar kept constant controlling for shelf placement is -3.205 grams.
> - ShelfMiddle: Controlling for sugar, rating for the middle shelf group is .79 points lower than the bottom shelf on average.
> - ShelfTop: Controlling for sugar, rating for the top shelf group is 4.69 points higher than the bottom shelf on average.
> - sugars_c: There is a decrease of -2.06 rating points for every one unit increase in sugar on average.
> - shelfMiddle:sugars_c: The slope for sugar on rating is .903 times higher for the middle shelf compared to the bottom shelf.
> - shelfTop:rating_c: The slope for sugar on rating is .447 times lower for the top shelf compared to the bottom shelf.

> - The original standard errors were 1.4 and 1.2 for the middle and top shelves while the interaction and rating were all around .1. The significant p values were for rating and middle shelf. The robust standard errors stayed the same for the middle shelf and slightly decreased for the top shelf but the interaction was now significant. After running coeftest, there is now a significant p value for the interaction of middle shelf and rating as well. 

> - Based on the adjusted r-squared value, we can estimate that rating and shelf level can explain about 55% of the variance in sugar per serving of cereal.

```{r}
#Question 4
#bootstrap SE

head(cer)
boot_dat<-cer[sample(nrow(cer),replace=TRUE),]
  fit2<-lm(rating_c~shelf*sugars_c,data=boot_dat) 
# repeat 5000 times
samp_distn<-replicate(5000, {
  boot_dat<-cer[sample(nrow(cer),replace=TRUE),]
  fit2<-lm(rating_c~shelf*sugars_c,data=boot_dat) 
  coef(fit2)
})
## Estimated SEs
samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)


```

After running bootstrap errors, the middle and top shelf increased in value compared to the original as well as the robust errors except for the Middle shelf. The interaction between sugar and middle shelf increased but only slightly. Since the SEs got bigger, we can assume that the p values also got bigger.

```{r}
#Question 5
fit3<- glm(mfr~sugars+rating, data = cer, family=binomial(link="logit"))
coeftest(fit3)
exp(coef(fit3))
prob<- predict(fit3, type = "response")

#confusion matrix
table(predict=as.numeric(prob>.5),truth=cer$mfr)%>%addmargins

#accuracy
(12+18)/43

#sensitivity
(12/17)

#specificity
18/26

#ppv
12/20

#plot densit of log odds
odds<-function(p)p/(1-p)
p<-seq(0,1,by=.1)
cbind(p, odds=odds(p))%>%round(4)

logit<-function(p)log(odds(p))
cbind(p, odds=odds(p),logit=logit(p))%>%round(4)
ggplot()+stat_function(aes(p),fun=logit,geom="line")+ylab("g(p)=logit(p)")+xlab("p")


# ROC curve
#install.packages("plotROC")
library(plotROC)

head(cer2)
ROCplot<-ggplot(cer2)+geom_roc(aes(d=mfr,m=prob), n.cuts=0)
ROCplot

#AUC
calc_auc(ROCplot)

#needed for function
class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}
#in sample performance
class_diag(prob, cer$mfr)
```

```{r}
#Question 5 continued
#repeated random subsampling
head(cer)
set.seed(1234)
fraction<-0.5 #choose proportion of rows to train
train_n<-floor(fraction*nrow(cer)) #number of rows to train
iter<-500 #number of iterations
diags<-NULL
for(i in 1:iter){
## Create training and test sets
train_index<-sample(1:nrow(cer),size=train_n)
train<-data[train_index,]
test<-data[-train_index,]
truth<-test$mfr
## Train model on training set (random half of dataset)
fit<-glm(mfr~rating,data=train,family="binomial")
## Test model on remaning half of dataset; get classification diagnostics
probs<-predict(fit,newdata = test,type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)
```
> - Intercept: odds of manufacterer being Kellogs when sugar and rating are held constant is 2.4e-6.
> - sugars: controlling for rating, for every one unit increase in sugar, odds of the manufacterer being Kellogs increase by a factor of 1.478
> - rating: controlling for sugar, for every one unit increase in rating, odds of the manufacterer being Kellogs increase by a factor of 1.304
> - The AUC is .759 which would fall in the Fair category.
> - The out of sample performance for accuracy is .605, for sensitivity is .669, for specificity is .570 and the AUC is .759.


```{r}
#Question 6

head(cer1)
cer1<- cer%>%select(-1)
#LASSO
#install.packages("glmnet")
library(glmnet)
#install.packages("plyr")
library(dplyr)

y<-as.matrix(cer1$mfr) #grab response
x<-model.matrix(mfr~.,data=cer1)[,-1] #grab predictors
head(x)


cv<-cv.glmnet(x,y, family = "binomial")
lasso<-glmnet(x,y,family = "binomial", lambda=cv$lambda.1se)
coef(lasso)
```

```{r}
#needed for function

set.seed(1234)
k=10
data <- cer2 %>% sample_frac #put rows of dataset in random order
folds <- ntile(1:nrow(data),n=10) #create fold labels
diags<-NULL
for(i in 1:k){
train <- data[folds!=i,] #create training set (all but fold i)
test <- data[folds==i,] #create test set (just fold i)
truth <- test$mfr #save truth labels from fold i
fit <- lm(mfr~rating,
data=train)
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth))
}
diags%>%summarize_all(mean)

```
The only variable that was retained was the rating variable. The AUC for question 5 was .82 and the out of sample performance was worse as the AUC was .7 and the accuracy dropped from .605 to .545. Since these values are lower, we can assume that we were overfitting.
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

